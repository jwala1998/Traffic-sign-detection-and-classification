{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23751,"status":"ok","timestamp":1702674105312,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"VJUxN_fpjNcQ","outputId":"0de63199-bcc1-48a1-834f-f0ed81b458e5"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6972,"status":"ok","timestamp":1702674116716,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"tAOoNP2rYTr-"},"outputs":[],"source":["import os\n","import random\n","import math\n","from datetime import datetime\n","from collections import Counter\n","import pandas as pd\n","import numpy as np\n","\n","import cv2\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","from sklearn.model_selection import train_test_split\n","import xml.etree.ElementTree as ET\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import models\n"]},{"cell_type":"markdown","metadata":{"id":"93tVF4FcY1R9"},"source":["**Formulation of the problem**\n","\n","For an image consisting of a road sign, predict the bounding box around the road sign and determine the type of road sign. These signs can belong to four different classes:\n","\n","Traffic light Stop Speed ​​Limit Crosswalk This is called a multi-task learning task because it involves performing two tasks:\n","1) regression to find the coordinates of the bounding box,\n","2) classification to determine the type of road sign.\n","\n","**Dataset**\n","It consists of 877 images. This is a fairly unbalanced dataset, most of the images are in the rate limit class, but since we are more focused on bounding box prediction, we can ignore the imbalance.\n","\n","**Loading data**\n","\n","Descriptions for each image are stored in separate XML files. Let's take the following steps to prepare data for training:\n","\n","Let's go through the annotations directory to get all the .xml files\n","Let's read the information we need from each .xml file using xml.etree.ElementTree\n","Let's create a dictionary containing filepath (path to the image), width, height, (xmin , xmax , ymin , ymax) (coordinates of the bounding box) and class and add the dictionary to the list.\n","Let's create a pandas dataframe using the list of dictionaries from the previous paragraph"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":199,"status":"ok","timestamp":1702674119797,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"UC_foUXXZMNq"},"outputs":[],"source":["images_path = Path('/content/drive/MyDrive/Fall 2023/ML/Project/traffic_Data/road-sign-detection/images')\n","anno_path = Path('/content/drive/MyDrive/Fall 2023/ML/Project/traffic_Data/road-sign-detection/annotations')\n","\n","#images_path = Path('/content/drive/MyDrive/ML project/Fall 2023/ML/Project/traffic_Data/road-sign-detection/images')\n","#anno_path = Path('/content/drive/MyDrive/ML project/Fall 2023/ML/Project/traffic_Data/road-sign-detection/annotations')\n","\n","\n","def filelist(root, file_type):\n","    \"\"\"The function returns a fully qualified list of files in a directory\"\"\"\n","    return [os.path.join(directory_path, f) for directory_path, directory_name,\n","            files in os.walk(root) for f in files if f.endswith(file_type)]\n","\n","def generate_train_df (anno_path):             #here it is creatextracting each images meta data and location of box in that image, this data used for training later\n","    annotations = filelist(anno_path, '.xml')\n","    print(annotations)\n","    anno_list = []\n","    for anno_path in annotations:\n","        root = ET.parse(anno_path).getroot()\n","        anno = {}\n","        anno['filename'] = Path(str(images_path) + '/'+ root.find(\"./filename\").text)\n","        anno['width'] = root.find(\"./size/width\").text\n","        anno['height'] = root.find(\"./size/height\").text\n","        anno['class'] = root.find(\"./object/name\").text\n","        anno['xmin'] = int(root.find(\"./object/bndbox/xmin\").text)\n","        anno['ymin'] = int(root.find(\"./object/bndbox/ymin\").text)\n","        anno['xmax'] = int(root.find(\"./object/bndbox/xmax\").text)\n","        anno['ymax'] = int(root.find(\"./object/bndbox/ymax\").text)\n","        anno_list.append(anno)\n","    return pd.DataFrame(anno_list)\n","\n","    print()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"executionInfo":{"elapsed":16067,"status":"ok","timestamp":1702674138862,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"XMHd8Vy7ZXgy","outputId":"83690090-9373-4809-809a-1c8d2f636d60"},"outputs":[],"source":["df_train = generate_train_df(anno_path)\n","df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1702674186881,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"22KZEmY7ZaVI","outputId":"581d9e1e-12df-417d-d571-01b0aa37c906"},"outputs":[],"source":["df_train['class'].value_counts()     #no.of unique classes"]},{"cell_type":"markdown","metadata":{"id":"2k3TiuB4ZfK_"},"source":["**Let's convert our labels into classes:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702674188597,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"O99LM_SAZha8","outputId":"ad987e5c-83a6-41d5-924e-248f21b2569b"},"outputs":[],"source":["class_dict = {'speedlimit': 0, 'stop': 1, 'crosswalk': 2, 'trafficlight': 3}\n","df_train['class'] = df_train['class'].apply(lambda x:  class_dict[x])   #changing string to int\n","\n","print(df_train.shape)\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"5jCrjIyXZphL"},"source":["**Resizing images and bounding boxes**\n","Since images must be the same size to train a computer vision model, we need to resize our images and their corresponding bounding boxes. Resizing an image is easy, but resizing a bounding box is a little more difficult because each rectangle depends on the image and its dimensions.\n","\n","Here's the basic idea:\n","\n","Let's transform the bounding box into an image (mask) of the same size as the image corresponding to this rectangle. This mask will simply have 0 for the background and 1 for the area covering the bounding box.\n","\n","Let's read the image first:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1702674191136,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"C9xegHJiZti7"},"outputs":[],"source":["def read_image(path):\n","    return cv2.cvtColor(cv2.imread(str(path)), cv2.COLOR_BGR2RGB)\n","\n","\n","def create_mask(bb, x):\n","    rows,cols,*_ = x.shape\n","    Y = np.zeros((rows, cols))\n","    bb = bb.astype(np.int)\n","    Y[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n","    return Y\n","\n","def mask_to_bb(Y):\n","    cols, rows = np.nonzero(Y)\n","    if len(cols) == 0:\n","        return np.zeros(4, dtype=np.float32)\n","    top_row = np.min(rows)\n","    left_col = np.min(cols)\n","    bottom_row = np.max(rows)\n","    right_col = np.max(cols)\n","    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n","\n","\n","def create_bb_array(x):\n","    return np.array([x[5],x[4],x[7],x[6]])\n","\n","def resize_image_bb(read_path, write_path, bb, sz):\n","    im = read_image(read_path)\n","    im_resized = cv2.resize(im, (sz, sz))\n","    Y_resized = cv2.resize(create_mask(bb, im), (sz, sz))\n","    new_path = str(write_path/read_path.parts[-1])\n","    cv2.imwrite(new_path, cv2.cvtColor(im_resized, cv2.COLOR_RGB2BGR))\n","    return new_path, mask_to_bb(Y_resized)"]},{"cell_type":"markdown","metadata":{"id":"QEnYz4gpZ6LP"},"source":["**Let's apply all our written functions:**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":550,"status":"ok","timestamp":1702674205547,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"lbTK42KTaCpl"},"outputs":[],"source":["IM_SIZE = 300"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"elapsed":608154,"status":"ok","timestamp":1702674814983,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"Wiz_locnaFbx","outputId":"dac7e2ae-7577-4f09-fe1c-1e1324e00256"},"outputs":[],"source":["#making all images the same size\n","new_paths = []\n","new_bbs = []\n","train_path_resized = Path('./images_resized')\n","Path.mkdir(train_path_resized, exist_ok=True)\n","\n","\n","for index, row in df_train.iterrows():\n","    new_path,new_bb = resize_image_bb(row['filename'], train_path_resized, create_bb_array(row.values), IM_SIZE)\n","    new_paths.append(new_path)\n","    new_bbs.append(new_bb)\n","\n","\n","df_train['new_path'] = new_paths   #saving new img location\n","df_train['new_bb'] = new_bbs       #saving new boundary data\n","\n","df_train.head()"]},{"cell_type":"markdown","metadata":{"id":"ezzy7b_oaLnw"},"source":["**Example of the resulting sample**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1702674844575,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"JwCpBwo7nmp_","outputId":"d2c039c0-472c-4b4b-ff10-86ee050ae5c2"},"outputs":[],"source":["print(torch.cuda.is_available())\n","num_gpus = torch.cuda.device_count()\n","print(num_gpus)\n","torch.cuda.set_device(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":559},"executionInfo":{"elapsed":1281,"status":"ok","timestamp":1702674847103,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"jPR-OLfoaNQb","outputId":"cd49a71e-a367-499c-8a7d-50f3db5654c7"},"outputs":[],"source":["#im = cv2.imread(str(df_train.values[30][0]))  somehow changed to 1\n","im = cv2.imread(str(df_train.values[30][0]))\n","print(df_train.values[30][0])\n","bb = create_bb_array(df_train.values[30])\n","\n","print(im.shape)\n","\n","Y = create_mask(bb, im)\n","mask_to_bb(Y)\n","\n","plt.imshow(im)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1702674850137,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"Px1mAVK9aRao","outputId":"bbe1d2d7-462d-4a22-c8d8-8e5f404051d0"},"outputs":[],"source":["plt.imshow(Y, cmap='gray')"]},{"cell_type":"markdown","metadata":{"id":"Dtd1NFK8aWDf"},"source":["**Data Augmentation**\n","Data augmentation is a technique that allows us to better generalize our model by creating new training images using different variations of existing images. Our current training set only has 800 images, so increasing the data is important to prevent our model from overfitting.\n","For this task we will use flip, rotate, center trim and random trim.\n","\n","The only thing to remember here is to make sure that the bounding box also transforms in the same way as the image. To do this, we follow the same approach as resizing - convert the bounding box to a mask, apply the same transformations to the mask as the original image, and extract the coordinates of the bounding box."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":720,"status":"ok","timestamp":1702674853079,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"KpxCXCbuaZB5"},"outputs":[],"source":["\n","def crop(im, r, c, target_r, target_c):\n","    return im[r:r+target_r, c:c+target_c]\n","\n","def center_crop(x, r_pix=8):\n","    r, c,*_ = x.shape\n","    c_pix = round(r_pix*c/r)\n","    return crop(x, r_pix, c_pix, r-2*r_pix, c-2*c_pix)\n","\n","def rotate_cv(im, deg, y=False, mode=cv2.BORDER_REFLECT):\n","\n","    r,c,*_ = im.shape\n","    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n","    if y:\n","        return cv2.warpAffine(im, M, (c, r), borderMode=cv2.BORDER_CONSTANT)\n","    return cv2.warpAffine(im, M, (c, r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS)\n","\n","def random_cropXY(x, Y, r_pix=8):\n","\n","    r, c,*_ = x.shape\n","    c_pix = round(r_pix * c/r)\n","    rand_r = random.uniform(0, 1)\n","    rand_c = random.uniform(0, 1)\n","    start_r = np.floor(2 * rand_r * r_pix).astype(int)\n","    start_c = np.floor(2 * rand_c * c_pix).astype(int)\n","    xx = crop(x, start_r, start_c, r - 2*r_pix, c - 2*c_pix)\n","    YY = crop(Y, start_r, start_c, r - 2*r_pix, c - 2*c_pix)\n","    return xx, YY\n","\n","\n","def transformsXY(path, bb, is_transforms):\n","    x = cv2.imread(str(path)).astype(np.float32)\n","    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB) / 255\n","    Y = create_mask(bb, x)\n","    if is_transforms:\n","        rdeg = (np.random.random()-.50) * 20\n","        x = rotate_cv(x, rdeg)\n","        Y = rotate_cv(Y, rdeg, y=True)\n","        if np.random.random() > 0.5:\n","          x = np.fliplr(x).copy()\n","          Y = np.fliplr(Y).copy()\n","        x, Y = random_cropXY(x, Y)\n","    else:\n","        x, Y = center_crop(x), center_crop(Y)\n","    return x, mask_to_bb(Y)\n","\n","def create_corner_rect(bb, color='red'):\n","    bb = np.array(bb, dtype=np.float32)\n","    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n","                         fill=False, lw=3)\n","\n","def create_corner_rect_pred(bb, color='green'):\n","    bb = np.array(bb, dtype=np.float32)\n","    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n","                         fill=False, lw=3)\n","\n","def show_corner_bb(im, bb):\n","    plt.imshow(im)\n","    plt.gca().add_patch(create_corner_rect(bb))"]},{"cell_type":"markdown","metadata":{"id":"dk5e4uVpak0w"},"source":["**Example image**\n","Original:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":858,"status":"ok","timestamp":1702674873019,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"RrJKonEzarHs","outputId":"db8152bc-c7ce-40bc-995a-21fdd6e8ebf7"},"outputs":[],"source":["number = 45\n","im = cv2.imread(str(df_train['new_path'].values[number]))\n","print(str(df_train.values[number][8]))\n","im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","show_corner_bb(im, df_train['new_bb'].values[number])"]},{"cell_type":"markdown","metadata":{"id":"oaSNLZJOatc9"},"source":["**After transformation:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":1118,"status":"ok","timestamp":1702674883822,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"KFjoHGK4asYW","outputId":"3f1aab1b-3a77-4b6e-b4af-3a9c2f662519"},"outputs":[],"source":["im, bb = transformsXY(str(df_train['new_path'].values[number]),\n","                      df_train['new_bb'].values[number],\n","                      is_transforms=True)\n","show_corner_bb(im, bb)"]},{"cell_type":"markdown","metadata":{"id":"3-W8BLNfa3oD"},"source":["**Dataset**\n","Now that we have our data additions, we can create a PyTorch dataset. We normalize the images using ImageNet statistics because we will be using a pre-trained ResNet model and applying data augmentation to our dataset during training."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":629,"status":"ok","timestamp":1702674889738,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"1K3YPnfRa3Yy"},"outputs":[],"source":["df_train = df_train.reset_index()\n","X = df_train[['new_path', 'new_bb']]    #storing new path list n corresponding bb in X\n","Y = df_train['class']                   #storing corresponding boxes in Y\n","\n","#spliting data\n","X_training, X_test, y_training, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.2, random_state=42)\n","\n","\n","def normalize(im):\n","    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n","    return (im - imagenet_stats[0]) / imagenet_stats[1]\n","\n","class RoadDataset(Dataset):\n","    def __init__(self, paths, bb, y, is_transforms=False):\n","        self.is_transforms = is_transforms\n","        self.paths = paths.values\n","        self.bb = bb.values\n","        self.y = y.values\n","\n","    def __len__(self):          #gives no of images\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx): #gives transformed image , box boundarirs and sign class\n","        path = self.paths[idx]\n","        y_class = self.y[idx]\n","        x, y_bb = transformsXY(path, self.bb[idx], self.is_transforms)\n","        x = normalize(x)\n","        x = np.rollaxis(x, 2)\n","        return x, y_class, y_bb\n","\n","train_ds = RoadDataset(X_train['new_path'], X_train['new_bb'], y_train, is_transforms=False) #making an object o the class\n","valid_ds = RoadDataset(X_val['new_path'], X_val['new_bb'], y_val) #making an object o the class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdERMtcjOVFe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"L1Xh66i48JoL"},"source":["**Let's load all this into our dataloader:**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":207,"status":"ok","timestamp":1702674904890,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"TRgRqd-68SBx"},"outputs":[],"source":["batch_size = 16\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)   #dataloader is where dat is loaded to train a nn\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"MjgjhB4m8UNI"},"source":["**Model Definition**\n","As a model we will use a very simple pre-trained resNet-34 model. Since we have two tasks here, there are two final layers - bounding box regression and image classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":526,"status":"ok","timestamp":1702678441427,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"4a1qOo7h8ZXJ"},"outputs":[],"source":["'''\n","class BB_model(nn.Module):\n","      #nn is pytorch ndel\n","\n","    def _init_(self):\n","        super(BB_model, self)._init_()\n","        resnet = models.resnet34(pretrained=True)   #loading pretrained resnet34 model 'from torchvision import models'  this library has it\n","\n","        layers = list(resnet.children())[:8]   #Retrieves the first 8 immediate child modules of the ResNet model (resnet) using the children() method. These modules typically correspond to the initial layers of the ResNet architecture, which include convolutional layers, batch normalization, and pooling layers.\n","        #new model for features\n","        self.features = nn.Sequential(*layers)   # Constructs a new sequential module (nn.Sequential) called features by using the first 8 layers obtained from the ResNet. The *layers syntax is used to unpack the list of layers and pass them as individual arguments to nn.Sequential.\n","        #new classifier model\n","        # self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4)) #Defines another sequential module called classifier containing a batch normalization layer (nn.BatchNorm1d(512)) followed by a linear layer (nn.Linear(512, 4)). This block is often used as a fully connected classifier. The input size to the linear layer is 512, and it outputs a tensor of size 4.\n","        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 256),nn.ReLU(), nn.linear(256,4))\n","        #new bb model\n","        #self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))   #Defines yet another sequential module called bb with the same structure as the classifier. It consists of a batch normalization layer followed by a linear layer.\n","        self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 256),nn.ReLU(), nn.linear(256,4))\n","        #so instead of making all these models like this we can make out own models using the layes that resnet uses and adding thes layers ourself to have own work\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = F.relu(x)\n","        x = nn.AdaptiveAvgPool2d((1,1))(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        #forward propofare the model and then pass those values that output of featurs to classifier model and bb model to get the op\n","        return self.classifier(x), self.bb(x)\n","'''\n","class BB_model(nn.Module):  #nn is pytorch nn model\n","\n","    def __init__(self):\n","        super(BB_model, self).__init__()\n","\n","        resnet = models.resnet34(pretrained=True)   #loading pretrained resnet34 model 'from torchvision import models'  this library has it\n","        # resnet = models.resnet34(pretrained=True)   #loading pretrained resnet34 model 'from torchvision import models'  this library has it\n","\n","        layers = list(resnet.children())[:8]   #Retrieves the first 8 immediate child modules of the ResNet model (resnet) using the children() method. These modules typically correspond to the initial layers of the ResNet architecture, which include convolutional layers, batch normalization, and pooling layers.\n","\n","        #new model for features\n","        self.features = nn.Sequential(*layers)   # Constructs a new sequential module (nn.Sequential) called features by using the first 8 layers obtained from the ResNet. The *layers syntax is used to unpack the list of layers and pass them as individual arguments to nn.Sequential.\n","\n","        #new classifier model\n","        self.classifier = nn.Sequential( nn.Linear(512, 4)) #Defines another sequential module called classifier containing a batch normalization layer (nn.BatchNorm1d(512)) followed by a linear layer (nn.Linear(512, 4)). This block is often used as a fully connected classifier. The input size to the linear layer is 512, and it outputs a tensor of size 4.\n","        #self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 256),nn.ReLU(), nn.Linear(256,128),nn.ReLU(), nn.Linear(128,4))\n","        #self.classifier = nn.Sequential(nn.Linear(512, 256),nn.ReLU(), nn.Linear(256,128),nn.ReLU(), nn.Linear(128,4))\n","\n","        #new bb model\n","        self.bb = nn.Sequential( nn.Linear(512, 4))   #Defines yet another sequential module called bb with the same structure as the classifier. It consists of a batch normalization layer followed by a linear layer.\n","        #self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 256),nn.ReLU(), nn.Linear(256,128),nn.ReLU(), nn.Linear(128,4))\n","        #self.bb = nn.Sequential( nn.Linear(512, 256),nn.ReLU(), nn.Linear(256,128),nn.ReLU(), nn.Linear(128,4))\n","\n","        #so instead of making all these models like this we can make out own models using the layes that resnet uses and adding thes layers ourself to have own work\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = F.relu(x)\n","        x = nn.AdaptiveAvgPool2d((1,1))(x)\n","        x = x.view(x.shape[0], -1)\n","\n","        #forward propofare the model and then pass those values that output of featurs to classifier model and bb model to get the op\n","        return self.classifier(x), self.bb(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2387,"status":"ok","timestamp":1702678449773,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"RVXYX8ST8dGs","outputId":"45cf3ac7-ea93-423d-c28d-7e72046f1b34"},"outputs":[],"source":["resnet = models.resnet34(pretrained=True)\n","# resnet = models.resnet34(pretrained=True)\n","\n","list(resnet.children())[:8]"]},{"cell_type":"markdown","metadata":{"id":"tSo4Ves98gt6"},"source":["**Training **\n","To calculate the loss, we need to take into account both the classification loss and the bounding box regression loss, so we use a combination of cross-entropy and L1 loss (the sum of all absolute differences between the ground truth and the predicted coordinates)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":953,"status":"ok","timestamp":1702678455132,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"TjlNn9dD8s_z"},"outputs":[],"source":["model = BB_model().cuda()  #line 4 #assigning variable to model\n","\n","params = [p for p in model.parameters() if p.requires_grad]  #saving model parameters in p\n","optimizer = torch.optim.Adam(params, lr=0.006)  #seting what optimizer to use\n","epochs = 100\n","# model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1702678458473,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"d7Iv7VHP9CXK"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","training_loss = []\n","validation_loss = []\n","\n","their_train_loss =[]\n","their_val_loss = []\n","\n","def train():\n","    best_loss = 10000.0\n","\n","\n","\n","\n","    for i in range(epochs):\n","        model.train() #from line 4\n","        total = 0\n","        sum_loss = 0\n","        for x, y_class, y_bb in train_dl:  #as remenber train_dl is the dataloader that holds all the image locations, bb and lables\n","            len_batch = y_class.shape[0]\n","            x = x.cuda().float()   #sending data to gpu\n","            y_class = y_class.cuda()\n","            y_bb = y_bb.cuda().float()\n","\n","            out_class, out_bb = model(x)  #sending x to model and get its class n bb prediction\n","\n","            # losses calculation\n","            loss_class = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n","            loss_bb = F.l1_loss(out_bb, y_bb, reduction=\"sum\")\n","\n","            #total loss\n","            loss = loss_class + loss_bb\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total += len_batch\n","            sum_loss += loss.item()\n","\n","        #training_loss.append([loss_class.detach().cpu().numpy(),loss_bb.detach().cpu().numpy(),loss.detach().cpu().numpy()])\n","\n","\n","\n","\n","        #that is done for each image  and model is trained and optimized\n","        train_loss = sum_loss / total\n","        their_train_loss.append(train_loss)\n","\n","\n","        # Eval\n","        model.eval()\n","        val_total = 0\n","        val_sum_loss = 0\n","        correct = 0\n","\n","        #and now that trained model is tested againest the validation dataset\n","        #note how there is no optimizer and step here\n","        for x, y_class, y_bb in valid_dl:\n","            len_batch = y_class.shape[0]\n","\n","            x = x.cuda().float()\n","            y_class = y_class.cuda()\n","            y_bb = y_bb.cuda().float()\n","            out_class, out_bb = model(x)\n","\n","            loss_class = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n","            loss_bb = F.l1_loss(out_bb, y_bb, reduction=\"sum\")\n","            loss = loss_class + loss_bb\n","\n","\n","            _, pred = torch.max(out_class, 1)      # prediction of classifier\n","            correct += (pred == y_class).sum().item() #accuracy\n","\n","\n","            val_sum_loss += loss.item()\n","            val_total += len_batch\n","\n","\n","        #validation_loss.append([loss_class.detach().cpu().numpy(),loss_bb.detach().cpu().numpy(),loss.detach().cpu().numpy()])\n","\n","        #update_plot(training_loss, validation_loss)\n","\n","        val_loss = val_sum_loss / val_total\n","        val_acc = correct / val_total\n","\n","        their_val_loss.append(val_loss)\n","\n","        if val_loss<best_loss:\n","          print(\"found better\")\n","          best_loss = val_loss\n","          model_path = '/content/drive/MyDrive/Fall 2023/ML/Project/loss_100epoch_NpT_ResNet_IT_NT_BLRM.pth'  # pt:prettrained NT no transform AL addted layer1 blRM - batch norm removed IT in training\n","\n","          # Save the model\n","          torch.save(model, model_path)\n","\n","\n","        print(f\"Epoch [{i+1}/{epochs}]. train_loss {train_loss:.3f} val_loss {val_loss:.3f} val_acc {val_acc:.3f}\")\n","        # break\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lZi5S20J9NdM"},"source":["**The trick:** after completing the training, we can change the training step and continue:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":567,"status":"ok","timestamp":1702678463026,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"-jd31pDd9PyG"},"outputs":[],"source":["for i, param_group in enumerate(optimizer.param_groups):\n","    param_group[\"lr\"] = 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":946215,"status":"ok","timestamp":1702679410653,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"5_hTWZQv9R2K","outputId":"a0f1c84a-8d8c-4c0b-8614-e68ca4af82b9"},"outputs":[],"source":["train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1702679417901,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"YPNl6XAabAYL","outputId":"cdf40fdf-2be2-4116-ae2d-b8904927bd80"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Assuming training_loss and validation_loss are your lists of lists\n","\n","# Sample data\n","#training_loss = [[1, 2, 3], [2, 3, 5], [3, 4, 7]]\n","#validation_loss = [[1, 3, 4], [2, 4, 6], [3, 5, 8]]\n","\n","# Extracting the third column (total loss) for plotting\n","training_loss_values = [item for item in their_train_loss]\n","validation_loss_values = [item for item in their_val_loss]\n","\n","# Plotting\n","plt.plot(training_loss_values, label='Training Loss', marker='o')\n","plt.plot(validation_loss_values, label='Validation Loss', marker='o')\n","\n","# Adding labels and title\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Epochs')\n","\n","# Adding legend\n","plt.legend()\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SdyC8JO7iM29"},"source":["Saving the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1GkXWr5iMmo"},"outputs":[],"source":["model_path = '/content/drive/MyDrive/ML project/Fall 2023/ML/Project/loss_100epoch_NpT_ResNet_AT_NT_BLRM.pth'\n","\n","# Save the model\n","torch.save(model, model_path)"]},{"cell_type":"markdown","metadata":{"id":"XIzPt2rK9Twp"},"source":["**Testing**"]},{"cell_type":"markdown","metadata":{"id":"eaug300bj-ae"},"source":["load model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6f88uWlj94m"},"outputs":[],"source":["#model_path = '/content/drive/MyDrive/Fall 2023/ML/Project/30epoch_pretained_ResNet_model_intraining.pth'\n","#model_path = '/content/drive/MyDrive/Fall 2023/ML/Project/30epoch_pretained_ResNet_model_aftertraining.pth'\n","# model_path = '/content/drive/MyDrive/Fall 2023/ML/Project/30epoch_pretained_ResNet_model_aftertraining.pth'\n","model_name=\"loss_100epoch_pT_ResNet_AT_NT_BLRM.pth\"\n","model_path = '/content/drive/MyDrive/ML project/Fall 2023/ML/Project/'+model_name\n","\n","\n","model = torch.load(model_path)"]},{"cell_type":"markdown","metadata":{"id":"bde9E6Hsrn8S"},"source":["trying to use test split data for the sake of testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jq9rrYxLrsWH"},"outputs":[],"source":["#load_data\n","test_ds = RoadDataset(X_test['new_path'], X_test['new_bb'], y_test, is_transforms=False)\n","\n","#so here is the DS to be used  go to cell/line 186"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IghJPnV9X1Y"},"outputs":[],"source":["'''\n","im = read_image('./images_resized/road789.png')\n","Path.mkdir(Path('./road_signs_test'), exist_ok=True)\n","cv2.imwrite('./road_signs_test/road789.jpg', cv2.cvtColor(im, cv2.COLOR_RGB2BGR))\n","'''\n","'''\n","#this is for checking for one single image\n","\n","image = cv2.imread('/content/drive/MyDrive/Fall 2023/ML/Project/traffic_Data/road_sign_det_test/road57.png')\n","im = cv2.resize(image,(300,300))\n","\n","Path.mkdir(Path('./road_signs_test'), exist_ok=True)\n","cv2.imwrite('./road_signs_test/road61.jpg', cv2.cvtColor(im, cv2.COLOR_RGB2BGR))\n","'''\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1702053018905,"user":{"displayName":"Aditya Kore","userId":"11911682948315827584"},"user_tz":300},"id":"wPhKdFpC9zGB","outputId":"d48abb0b-029b-41a9-fa11-d92b18420ef7"},"outputs":[],"source":["class_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsJZN84DxrW0"},"outputs":[],"source":["def is_black(pixel):\n","    if pixel[0] == 0 and pixel[1] == 0 and pixel[2] == 0:\n","        return True\n","    else:\n","        return False\n","\n","def is_white(pixel):\n","    if pixel[0] == 255 and pixel[1] == 255 and pixel[2] == 255:\n","        return True\n","    else:\n","        return False\n","def mask(rectangle_coords, image_shape):\n","    # rectangle_coords is a tuple representing diagonal coordinates (x1, y1, x2, y2) of the rectangle\n","    # image_shape is a tuple representing the shape of the original image (height, width)\n","\n","    # Create an empty black image (mask)\n","    # mask = np.zeros(image_shape, dtype=np.uint8)\n","    mask = np.zeros((image_shape[0], image_shape[1], 3), dtype=np.uint8)\n","\n","    # Determine the coordinates of the rectangle\n","    # x1, y1, x2, y2 = rectangle_coords\n","    x1=int(rectangle_coords[0])\n","    y1=int(rectangle_coords[1])\n","    x2=int(rectangle_coords[2])\n","    y2=int(rectangle_coords[3])\n","\n","\n","\n","    # Draw a white rectangle on the mask\n","    mask=cv2.rectangle(mask, (x1, y1), (x2, y2), (255, 255, 255), thickness=cv2.FILLED)\n","\n","\n","    return mask\n","\n","def IoU(image, bb_pred, bb_gt):\n","# load predicted img and gt_img\n","        # img = cv2.imread(os.path.join(folder, file))\n","        # #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        # gt = cv2.imread(os.path.join(gt_folder, gt_name))\n","\n","        img=mask(bb_pred,(image.shape[0],image.shape[1]))\n","        # img= create_mask(bb_pred,image)\n","        gt=mask(bb_gt,(image.shape[0],image.shape[1]))\n","        # gt= create_mask(bb_gt,image)\n","\n","\n","        #gt = cv2.cvtColor(gt, cv2.COLOR_BGRA2RGB)\n","\n","        #resize gt image\n","        # print(img.shape)\n","        height, width,_= img.shape\n","        gt = cv2.resize(gt, (width, height))\n","        intersect = 0.0\n","        union = 0.0\n","        # iou=0\n","\n","        # count total number of gray pixels\n","        for i in range(height):\n","            for j in range(width):\n","                # if intersection\n","                if is_white(img[i, j]) and is_white(gt[i, j]):\n","                    intersect += 1\n","                if is_white(img[i, j]) or is_black(gt[i, j]):\n","                    union += 1\n","\n","        # if union != 0:\n","            # count += 1\n","            # iou += intersect / union\n","        rectangle_coords=bb_gt\n","        x1=int(rectangle_coords[0])\n","        y1=int(rectangle_coords[1])\n","        x2=int(rectangle_coords[2])\n","        y2=int(rectangle_coords[3])\n","\n","        iou= intersect / (abs(x1-x2)*abs(y1-y2))\n","\n","        return iou"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":333139,"status":"ok","timestamp":1702065390402,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"},"user_tz":300},"id":"07tb61mG9a5u","outputId":"c27cd95e-f586-4253-c7cd-ec4e2bbf4dc8"},"outputs":[],"source":["# test Dataset\n","from IPython.display import display, Image\n","'''\n","test_ds = RoadDataset(\n","    pd.DataFrame([{'path':'./road_signs_test/road61.jpg'}])['path'],\n","    pd.DataFrame([{'bb':np.array([0,0,0,0])}])['bb'],\n","    pd.DataFrame([{'y':[0]}])['y']\n",")\n","'''\n","#see how here test_ds[0]is used for one, similarly loop through all, y_class and y_bb has grouftruth values for label n bb and x is the image value to be sent to model to get results\n","#good luck\n","\n","#select what to test\n","\n","correct_labled = 0\n","\n","iou=0\n","k=0\n","\n","for i,n in enumerate(test_ds):\n","\n","  im_path = X_test['new_path'].values[i]\n","  image = cv2.imread(im_path)\n","  im = cv2.resize(image, (300,300))\n","  x, y_class, y_bb = n\n","\n","\n","\n","  xx = torch.FloatTensor(x[None,])\n","\n","  # prediction\n","  out_class, out_bb = model(xx.cuda())\n","\n","  # predicted class\n","  #y_pred = torch.max(out_class, 1)\n","  out_class =out_class.detach().cpu().numpy()\n","  y_pred = np.argmax(out_class)\n","  print(\"real value=\",y_class,\"Pred for\"+str(i)+\"=\",y_pred)\n","  # predicted bounding box\n","\n","  bb_hat = out_bb.detach().cpu().numpy()\n","  bb_hat = bb_hat.astype(int)\n","\n","  temp=IoU(im,bb_hat.flatten(),y_bb)\n","  k=k+1\n","  iou+=temp\n","  print(\"Intersected_fraction=\"+str(temp))\n","\n","\n","  if y_pred == y_class:\n","    correct_labled+=1\n","\n","\n","  # print(\"original label was:\", y_class)\n","  # print(\"predicted label was:\", y_pred)\n","\n","  # print(\"original bb was:\", y_bb)\n","  # print(\"predicted bb was:\", bb_hat)\n","  text1=\"real value=\"+str(y_class)+\"  Pred for\"+str(i)+\"=\"+str(y_pred)\n","  text2=\"Intersected_fraction=\"+str(temp)\n","  plt.imshow(im)\n","  plt.gca().add_patch(create_corner_rect_pred(bb_hat[0]))\n","  plt.gca().add_patch(create_corner_rect(y_bb))\n","\n","  text_x = (y_bb[0] + y_bb[2]) / 2\n","  text_y = y_bb[1] - 5  # Adjust the y-coordinate to position the text above the rectangle\n","  plt.text(0, 2, text1, color='blue', ha='left', va='top')\n","  plt.text(0, 12, text2, color='blue', ha='left', va='top')\n","\n","  plt.savefig(\"/content/drive/MyDrive/ML project/100epoch_pT_ResNet_AT_NT_BLRM/\"+str(i))\n","  plt.show()  # Display the current image\n","\n","\n","\n","\n","# test_loss_class = loss_class/count\n","# test_bb_class = loss_bb/count\n","\n","accuracy = correct_labled/(i+1)\n","\n","print(\"---------------------------------------------\")\n","print(\"total average class label accuracy:\" , accuracy)\n","avg_intersected_fraction= iou/k\n","\n","print(\"Average Intersected area=\",avg_intersected_fraction)\n","\n","# print(\"total average class loss:\" , test_loss_class)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1702063685195,"user":{"displayName":"Jwalandhar Girnar","userId":"07935721020492465045"},"user_tz":300},"id":"Q4lBmRgN_Bhi","outputId":"fe175ea4-5d68-45a6-bb9a-e27584b500ac"},"outputs":[],"source":["print(\"Average Intersected area=\",avg_intersected_fraction)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
